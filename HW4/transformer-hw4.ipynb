{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Attention Is All You Need\n\n## About this Notebook\nThis notebook focuses on training a Transformer model to translate sentences from French to English. The Transformer model, proposed in the paper \"Attention is All You Need\" by Vaswani et al.\n\nWe refer to the article [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/#embeddings-and-softmax) for the implementation details of the Transformer model.\n\n## Dataset\nWe use the [English-French Translation Dataset](https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset/) available on Kaggle for training this model. This dataset contains more than 22.5 million sentence pairs for translation. However, due to computational constraints and to avoid timeout during training on Kaggle, we only train on 1% of the dataset.\n\n## Training\nThe training is performed using **2 x T4 GPUs** with PyTorch's `DataParallel` for distributed computing.\n","metadata":{"id":"3PXhPnuAt6vv"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import LambdaLR\nimport math\nimport numpy as np\nimport unicodedata\nimport string\nimport re\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device :\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"Ij1xZ742t6vy","outputId":"079a5f12-cb1e-47d6-9c1e-1425c9a2a0b5","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-25T09:25:11.569257Z","iopub.execute_input":"2025-12-25T09:25:11.569960Z","iopub.status.idle":"2025-12-25T09:25:19.301229Z","shell.execute_reply.started":"2025-12-25T09:25:11.569918Z","shell.execute_reply":"2025-12-25T09:25:19.300270Z"}},"outputs":[{"name":"stdout","text":"device : cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Text Preprocessing ðŸ¤–\n\nNLP involves a lot of data wrangling. To stick to the simple but insightful example I won't use the usual convenience functions (for example by hugginface) to create the text-pipeline.\nTokenizer, Vocabulary, Batch-Makers are coded with good ol' python.\n\n*The code for the tokenizer and vocab builder are from [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) excellent torch tutorial!*","metadata":{"id":"O-qDHjvkt6v0"}},{"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\nPAD_token = 2\nMAX_LENGTH = 50\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"}\n        self.n_words = 3  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\n# def readLangs(lang1, lang2, reverse=False):\n#     lines = open('../input/english-french/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n#         read().strip().split('\\n')\n\n#     # Split every line into pairs and normalize\n#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n#     # Reverse pairs, make Lang instances\n#     if reverse:\n#         pairs = [list(reversed(p)) for p in pairs]\n#         input_lang = Lang(lang2)\n#         output_lang = Lang(lang1)\n#     else:\n#         input_lang = Lang(lang1)\n#         output_lang = Lang(lang2)\n\n#     return input_lang, output_lang, pairs\n\ndef readLangs(lang1, lang2, reverse=False):\n    df = pd.read_csv('/kaggle/input/en-fr-translation-dataset/en-fr.csv')\n\n    # Sample 1% of your dataframe\n    df_sample = df.sample(frac=0.01)\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(str(s)) for s in l] for l in df_sample.values]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:25:19.302830Z","iopub.execute_input":"2025-12-25T09:25:19.303213Z","iopub.status.idle":"2025-12-25T09:25:19.313488Z","shell.execute_reply.started":"2025-12-25T09:25:19.303189Z","shell.execute_reply":"2025-12-25T09:25:19.312800Z"},"trusted":true,"id":"-MsZ3K8Yt6v1"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepareData('eng', 'fra', True)\nprint(random.choice(pairs))","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:25:19.314780Z","iopub.execute_input":"2025-12-25T09:25:19.315081Z","iopub.status.idle":"2025-12-25T09:28:15.173665Z","shell.execute_reply.started":"2025-12-25T09:25:19.315051Z","shell.execute_reply":"2025-12-25T09:28:15.173070Z"},"trusted":true,"id":"ZVgtWwCst6v2","outputId":"63d95ff9-d844-475f-ba34-9c211c9ac61b","colab":{"base_uri":"https://localhost:8080/","height":356}},"outputs":[{"name":"stdout","text":"Read 225204 sentence pairs\nTrimmed to 192648 sentence pairs\nCounting words...\nCounted words:\nfra 99128\neng 83996\n['il est tue la meme annee lors de son deuxieme voyage en pays iroquois .', 'he was murdered that year on his second trip into iroquois country .']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### From sentence to indexes and batches\n\nBellow are the functions that transform sentences into lists of indexes and then creates batches.","metadata":{"id":"sa9xAD0Bt6v2"}},{"cell_type":"code","source":"def indexes_from_sentence(lang, sentence):\n    idxs = [lang.word2index[word] for word in sentence.split(' ')]\n    idxs.append(EOS_token)\n    idxs.insert(SOS_token,0)\n    return idxs\n\ndef batch_from_pairs(pairs):\n    batch_inp = [indexes_from_sentence(input_lang, p[0]) for p in pairs]\n    longest_seq = max([len(seq) for seq in batch_inp])\n    batch_inp = [seq+[PAD_token]*(longest_seq-len(seq)) for seq in batch_inp]\n    input_tensor = torch.tensor(batch_inp, dtype=torch.long, device=device)\n\n    batch_trg = [indexes_from_sentence(output_lang, p[1]) for p in pairs]\n    longest_seq = max([len(seq) for seq in batch_trg])\n    batch_trg = [seq+[PAD_token]*(longest_seq-len(seq)) for seq in batch_trg]\n    target_tensor = torch.tensor(batch_trg, dtype=torch.long, device=device)\n\n    return input_tensor,target_tensor","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.175150Z","iopub.execute_input":"2025-12-25T09:28:15.175374Z","iopub.status.idle":"2025-12-25T09:28:15.184482Z","shell.execute_reply.started":"2025-12-25T09:28:15.175353Z","shell.execute_reply":"2025-12-25T09:28:15.183858Z"},"trusted":true,"id":"Ga4JR1itt6v3"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Implementing the Transformer","metadata":{"id":"WSBhSL5At6v3"}},{"cell_type":"markdown","source":"![Model Architecture](https://www.researchgate.net/profile/Dennis-Gannon-2/publication/339390384/figure/fig1/AS:860759328321536@1582232424168/The-transformer-model-from-Attention-is-all-you-need-Viswani-et-al.jpg)\n\n### Some further notes to the illustration\n\n* It is encoder-decoder architecture.\n* The encoder creates features of the input sequence.\n* The decoder uses the output of the encoder to decode the target sequence.\n* The decoder uses the shifted targets during training. During prediction the outputs are fed back as the new inputs.\n* Both encoder are using stacked self-attention and point-wise, fully connected layers.\n\nLet's go through it step by step.","metadata":{"id":"97uIIY3wt6v3"}},{"cell_type":"markdown","source":"###  Scaled dot-product attention\nThe authors wrote:\n\n> We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by âˆš\ndk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\n\n![Attention function](https://miro.medium.com/max/720/1*P9sV1xXM10t943bXy_G9yg.png)\n\n### Multihead attention\n\n> Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the final values ... Multi-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n","metadata":{"id":"3DDiqVfJt6v3"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement Scaled Dot-Product Attention\n############################################\ndef attention(q, k, v, dropout, mask=None):\n    \"\"\"\n    Implement the scaled dot-product attention mechanism.\n\n    Steps to implement:\n    1. Compute raw attention scores using matrix multiplication: q @ káµ€\n    2. Scale the scores by dividing by sqrt(d_k)\n    3. If a mask is provided, set masked positions to a very negative value\n       so they become zero after softmax.\n    4. Apply softmax over the last dimension to obtain attention weights.\n    5. Apply dropout to the attention weights.\n    6. Multiply the attention weights by v to obtain the output.\n\n    Return:\n        The weighted sum of values.\n    \"\"\"\n\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    p_attn = F.softmax(scores, dim=-1)\n\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n\n    return torch.matmul(p_attn, v)\n\n\n############################################\n# STUDENT TASK: Implement Multi-Head Attention\n############################################\nclass MultiHeaderAttention(nn.Module):\n    \"\"\"\n    Multi-Head Attention layer used in Transformer models.\n\n    Students must:\n    1. Complete all linear projections for q, k, v.\n    2. Reshape the projections into multiple heads.\n    3. Apply the attention() function across all heads.\n    4. Concatenate the heads and apply the final output projection.\n    5. Properly handle masking across heads.\n    \"\"\"\n\n    def __init__(self, d_model, dropout, n_heads=8, dk=64, dv=64):\n        super(MultiHeaderAttention, self).__init__()\n        assert d_model % n_heads == 0\n\n        # STUDENT TASK:\n        # Define linear layers to project inputs into q, k, v vectors\n        # Each should project to (n_heads * dk) or (n_heads * dv)\n        # Also define dropout and the final output linear layer.\n        self.d_k = d_model // n_heads\n        self.n_heads = n_heads\n        self.linears = nn.ModuleList([\n            nn.Linear(d_model, d_model),  # Query\n            nn.Linear(d_model, d_model),  # Key\n            nn.Linear(d_model, d_model),  # Value\n            nn.Linear(d_model, d_model)   # Output\n        ])\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, k, v, q, mask=None):\n        \"\"\"\n        Forward pass of Multi-Head Attention.\n\n        Steps to implement:\n        1. Get batch size and sequence lengths.\n        2. Apply the q, k, v linear projections.\n        3. Reshape each projected tensor to (batch, heads, seq_len, dk or dv).\n        4. Move the head dimension before sequence length.\n        5. Apply the attention() function over heads.\n        6. Re-combine heads by transposing back and reshaping.\n        7. Apply the final output projection.\n        8. Apply mask correctly by expanding across heads.\n        \"\"\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n\n        nbatches = q.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        q, k, v = [\n            l(x).view(nbatches, -1, self.n_heads, self.d_k).transpose(1, 2)\n            for l, x in zip(self.linears, (q, k, v))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x = attention(q, k, v, self.dropout, mask=mask)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = (\n            x.transpose(1, 2)\n            .contiguous()\n            .view(nbatches, -1, self.n_heads * self.d_k)\n        )\n        return self.linears[-1](x)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.185376Z","iopub.execute_input":"2025-12-25T09:28:15.185562Z","iopub.status.idle":"2025-12-25T09:28:15.213898Z","shell.execute_reply.started":"2025-12-25T09:28:15.185544Z","shell.execute_reply":"2025-12-25T09:28:15.213389Z"},"trusted":true,"id":"vgWNBiFUt6v4"},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Positional encoding\n\nThe model needs to know the order of the words in a sentence. (Or more precisely, the order of the embeddings)\nIn the paper they do this by adding a cyclical signal to the word embeddings.\n\nThe authors write:\n> Since our model contains no recurrence and no convolution, in order for the model to make use of the\n> order of the sequence, we must inject some information about the relative or absolute position of the\"\n\nand\n\n> To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000* 2i/dmodel) PE(pos,2i+1) = cos(pos/10000 * 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000 Â· 2Ï€.","metadata":{"id":"KJ7NCg9Bt6v4"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\n############################################\n# STUDENT TASK: Implement Positional Encoding\n############################################\nclass PositionEncoding(nn.Module):\n    \"\"\"\n    Absolute positional encoding used in Transformer models.\n\n    Students must complete:\n    - pos_encoding(): compute sinusoidal positional embedding for a given position.\n    - tensor_pos_encoding(): generate a table of embeddings for all positions.\n    - forward(): add positional embeddings to the input.\n    \"\"\"\n\n    def __init__(self, max_len, d_model):\n        super(PositionEncoding, self).__init__()\n\n        # 1. Store max_len\n        self.max_len = max_len\n        self.d_model = d_model\n\n        # 2. Register a buffer named \"pos_table\" containing the\n        #    precomputed positional encodings for all positions.\n        # Hint: use self.register_buffer(name, tensor)\n        self.register_buffer('pos_table', self.tensor_pos_encoding(max_len, d_model))\n\n    def pos_encoding(self, pos, k):\n        \"\"\"\n        STUDENT TASK:\n\n        Compute the sinusoidal positional encoding for a single position `pos`\n        and dimension `k`.\n\n        For dimension index i (0 â‰¤ i < k):\n            If i is even:   use sin( pos / (10000^(i/k)) )\n            If i is odd:    use cos( pos / (10000^(i/k)) )\n\n        Return:\n            A list (or tensor) of length k containing the positional embedding.\n        \"\"\"\n        pe = torch.zeros(k)\n        position = torch.tensor(pos, dtype=torch.float)\n        div_term = torch.exp(torch.arange(0, k, 2).float() * -(math.log(10000.0) / k))\n\n        pe[0::2] = torch.sin(position * div_term)\n        pe[1::2] = torch.cos(position * div_term)\n        return pe\n\n    def tensor_pos_encoding(self, max_len, dim):\n        \"\"\"\n        STUDENT TASK:\n\n        Build the full positional encoding table:\n        - Shape: (max_len, dim)\n        - Each row i contains the positional encoding for position i.\n\n        Return:\n            A tensor containing all positional encodings.\n        \"\"\"\n        pos_table = torch.zeros(max_len, dim)\n        for pos in range(max_len):\n            pos_table[pos, :] = self.pos_encoding(pos, dim)\n        return pos_table.unsqueeze(0) # Add a batch dimension (1, max_len, dim) for easy broadcasting\n\n    def forward(self, x):\n        \"\"\"\n        STUDENT TASK:\n\n        Add positional embeddings to the input tensor.\n\n        Steps:\n        1. Extract the sequence length from x.\n        2. Select the corresponding rows from pos_table.\n        3. Expand positional encodings to match batch size.\n        4. Add them to x and return the result.\n\n        Note:\n        - Remember to detach() the table to avoid gradient computation.\n        \"\"\"\n        seq_len = x.size(1)\n        # Select the corresponding rows from pos_table and detach to avoid gradient computation\n        # self.pos_table has shape (1, max_len, d_model)\n        # We need (batch_size, seq_len, d_model)\n        # Slicing will give (1, seq_len, d_model), which broadcasts correctly with x (batch_size, seq_len, d_model)\n        return x + self.pos_table[:, :seq_len].detach()\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.214698Z","iopub.execute_input":"2025-12-25T09:28:15.214984Z","iopub.status.idle":"2025-12-25T09:28:15.240164Z","shell.execute_reply.started":"2025-12-25T09:28:15.214956Z","shell.execute_reply":"2025-12-25T09:28:15.239601Z"},"trusted":true,"id":"t_3-17SDt6v5"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Subylayer connection\n\nSometimes it's good to learn from the past. That's basically what residual connections do.\n\n> We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself.\n\nConcering dropout:\n> We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.","metadata":{"id":"nSfcaL_dt6v5"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement SublayerConnection\n############################################\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    Implements the 'Add & Norm' operation used throughout the Transformer.\n\n    Students must implement:\n    - A LayerNorm operation\n    - A Dropout layer\n    - A forward() method that:\n        1. Applies LayerNorm to the input\n        2. Passes the normalized input through a given sublayer (function)\n        3. Applies dropout\n        4. Adds the original input (residual connection)\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n\n        # STUDENT TASK:\n        # 1. Create a LayerNorm of dimension `size`\n        # 2. Create a Dropout layer with probability `dropout`\n        self.norm = nn.LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"\"\"\n        STUDENT TASK:\n\n        Implement the forward pass:\n\n        Steps:\n        1. Normalize input x using the layer norm.\n        2. Pass normalized x to the provided `sublayer` function.\n           (The sublayer will typically be attention or a feed-forward network.)\n        3. Apply dropout to the sublayerâ€™s output.\n        4. Add the result back to the original input (residual connection).\n\n        Return:\n            The output of x + dropout(sublayer(norm(x)))\n        \"\"\"\n        return x + self.dropout(sublayer(self.norm(x)))\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.241303Z","iopub.execute_input":"2025-12-25T09:28:15.241600Z","iopub.status.idle":"2025-12-25T09:28:15.260995Z","shell.execute_reply.started":"2025-12-25T09:28:15.241580Z","shell.execute_reply":"2025-12-25T09:28:15.260469Z"},"trusted":true,"id":"u-dxfLibt6v5"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Encoder\n\nThe encoder will take our French sentences an *encode* it. Imagine that you are engineering features for your favorite gradient booster model. The output of the encoder is basically the features of our French sentences, ready to be fed into the decoder.\n\nThe authors description of the encoder:\n> The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\n\nThe application of attention in the encoder is described as followed:\n> The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.","metadata":{"id":"d8afO1Y1t6v5"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement the Transformer Encoder\n############################################\nclass Encoder(nn.Module):\n    \"\"\"\n    Full Transformer Encoder consisting of:\n    - Token embeddings\n    - Positional encodings\n    - N stacked EncoderLayers\n\n    Students must implement:\n    1. The embedding layer for input tokens\n    2. The positional encoding addition\n    3. The loop over N encoder layers\n    4. A final LayerNorm\n    \"\"\"\n\n    def __init__(self, n_input_vocab, d_model, n_hidden, n_layers, dropout):\n        super().__init__()\n\n        # STUDENT TASK:\n        # 1. Create an embedding layer of size (n_input_vocab, d_model)\n        #    - Remember to specify padding_idx\n        # 2. Add a dropout layer\n        # 3. Add a final LayerNorm for output normalization\n        # 4. Create a list of n_layers EncoderLayer modules\n        # 5. Instantiate a PositionalEncoding module\n        self.d_model = d_model\n        self.embedding = nn.Embedding(n_input_vocab, d_model, padding_idx=PAD_token)\n        self.pe = PositionEncoding(max_len=MAX_LENGTH + 2, d_model=d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, n_hidden, dropout) for _ in range(n_layers)])\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, mask):\n        \"\"\"\n        STUDENT TASK:\n\n        Steps:\n        1. Convert token indices into embeddings.\n        2. Scale embeddings by sqrt(d_model).\n        3. Add positional encodings.\n        4. Pass through each EncoderLayer sequentially.\n        5. Apply final normalization.\n\n        Arguments:\n            x: (batch_size, sequence_length)\n            mask: (batch_size, sequence_length) or broadcastable\n\n        Return:\n            Encoded representation of shape (batch_size, sequence_length, d_model)\n        \"\"\"\n        # 1. Convert token indices into embeddings.\n        x = self.embedding(x)\n\n        # 2. Scale embeddings by sqrt(d_model).\n        x = x * math.sqrt(self.d_model)\n\n        # 3. Add positional encodings.\n        x = self.pe(x)\n        x = self.dropout(x)\n\n        # 4. Pass through each EncoderLayer sequentially.\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        # 5. Apply final normalization.\n        return self.norm(x)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.261794Z","iopub.execute_input":"2025-12-25T09:28:15.261970Z","iopub.status.idle":"2025-12-25T09:28:15.276384Z","shell.execute_reply.started":"2025-12-25T09:28:15.261953Z","shell.execute_reply":"2025-12-25T09:28:15.275668Z"},"trusted":true,"id":"Xuk6U1w_t6v6"},"outputs":[],"execution_count":9},{"cell_type":"code","source":"############################################\n# STUDENT TASK: Implement a single Encoder Layer\n############################################\nclass EncoderLayer(nn.Module):\n    \"\"\"\n    One layer of the Transformer Encoder.\n\n    This layer contains:\n    - Multi-head self-attention sublayer\n    - Feed-forward neural network sublayer\n    - Two SublayerConnection modules\n    \"\"\"\n\n    def __init__(self, d_model, n_hidden, dropout):\n        super(EncoderLayer, self).__init__()\n\n        # STUDENT TASK:\n        # 1. Create a MultiHeaderAttention module for self-attention\n        # 2. Create a feed-forward network:\n        #       Linear(d_model â†’ n_hidden)\n        #       ReLU\n        #       Dropout\n        #       Linear(n_hidden â†’ d_model)\n        # 3. Create two SublayerConnection modules:\n        #       - One for self-attention\n        #       - One for feed-forward\n        self.self_attn = MultiHeaderAttention(d_model, dropout)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, n_hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(n_hidden, d_model)\n        )\n        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n\n    def forward(self, x, mask):\n        \"\"\"\n        STUDENT TASK:\n\n        Forward pass consists of:\n        1. Applying self-attention inside a SublayerConnection:\n            sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n\n        2. Applying feed-forward inside the second SublayerConnection:\n            sublayer[1](output_of_step_1, self.feed_forward)\n\n        Return:\n            The transformed output tensor.\n        \"\"\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)\n","metadata":{"trusted":true,"id":"Ch1Ne6oat6v6","execution":{"iopub.status.busy":"2025-12-25T09:28:15.277835Z","iopub.execute_input":"2025-12-25T09:28:15.278423Z","iopub.status.idle":"2025-12-25T09:28:15.296082Z","shell.execute_reply.started":"2025-12-25T09:28:15.278391Z","shell.execute_reply":"2025-12-25T09:28:15.295563Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Decoder\n\nThe decoder will take the encoder output (features of the French sentences) and the target sequence (here the English sentences) and decode it. The decoder output is then fed into a linear layer whose job it is to predict the next word of each vector of the sequence. (See how the labels are made during training to understand why.)\n\n> The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n\n\nThe first application of the attention in the decoder (sublayer1) is described as follows:\n> Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n\nThe second application of the attention in the decoder (sublayer2) is described as follows:\n> In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence.","metadata":{"id":"mb7uKKntt6v6"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement the Transformer Decoder\n############################################\nclass Decoder(nn.Module):\n    \"\"\"\n    Transformer Decoder:\n    - Token embedding\n    - Positional encoding\n    - N stacked DecoderLayers\n    - Final LayerNorm\n\n    Students must:\n    1. Create an embedding layer for target vocabulary.\n    2. Add positional encodings.\n    3. Stack multiple DecoderLayer modules.\n    4. Apply LayerNorm to the final output.\n    \"\"\"\n\n    def __init__(self, n_target_vocab, d_model, n_hidden, n_layers, dropout):\n        super().__init__()\n\n        # STUDENT TASK:\n        # 1. Create target token embedding (with padding index).\n        # 2. Create dropout and final layer norm.\n        # 3. Create a ModuleList of n_layers DecoderLayer objects.\n        # 4. Create a PositionalEncoding module.\n        self.d_model = d_model\n        self.embedding = nn.Embedding(n_target_vocab, d_model, padding_idx=PAD_token)\n        self.pe = PositionEncoding(max_len=MAX_LENGTH, d_model=d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.layers = nn.ModuleList([DecoderLayer(d_model, n_hidden, dropout) for _ in range(n_layers)])\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, encoder_outputs, self_attn_mask, enc_dec_mask):\n        \"\"\"\n        STUDENT TASK:\n\n        Steps:\n        1. Convert target tokens to embeddings.\n        2. Scale embeddings by sqrt(d_model).\n        3. Add positional encodings.\n        4. Pass the sequence through each decoder layer:\n            - Each layer performs masked self-attention\n            - Then encoder-decoder attention\n            - Then a feed-forward network\n        5. Apply final LayerNorm.\n\n        Arguments:\n            x: target token indices (batch, tgt_len)\n            encoder_outputs: (batch, src_len, d_model)\n            self_attn_mask: target-side future mask (causal)\n            enc_dec_mask: encoder-to-decoder attention mask\n        \"\"\"\n        # 1. Convert target tokens to embeddings.\n        x = self.embedding(x)\n\n        # 2. Scale embeddings by sqrt(d_model).\n        x = x * math.sqrt(self.d_model)\n\n        # 3. Add positional encodings.\n        x = self.pe(x)\n        x = self.dropout(x)\n\n        # 4. Pass the sequence through each decoder layer:\n        for layer in self.layers:\n            x = layer(x, encoder_outputs, self_attn_mask, enc_dec_mask)\n\n        # 5. Apply final LayerNorm.\n        return self.norm(x)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.298084Z","iopub.execute_input":"2025-12-25T09:28:15.298700Z","iopub.status.idle":"2025-12-25T09:28:15.319408Z","shell.execute_reply.started":"2025-12-25T09:28:15.298670Z","shell.execute_reply":"2025-12-25T09:28:15.318903Z"},"trusted":true,"id":"9YKRSOpmt6v7"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement a single Decoder Layer\n############################################\nclass DecoderLayer(nn.Module):\n    \"\"\"\n    One layer of the Transformer Decoder.\n\n    Contains three sublayers:\n    1. Masked self-attention (target â†’ target)\n    2. Encoder-decoder attention (target â†’ source)\n    3. Feed-forward network\n\n    Each sublayer is wrapped in SublayerConnection.\n    \"\"\"\n\n    def __init__(self, d_model, n_hidden, dropout):\n        super(DecoderLayer, self).__init__()\n\n        # STUDENT TASK:\n        # 1. Create MultiHeaderAttention for:\n        #       - self-attention (target attends to target)\n        #       - encoder-decoder attention (target attends to memory)\n        # 2. Create a feed-forward network:\n        #       Linear(d_model â†’ n_hidden)\n        #       ReLU\n        #       Dropout\n        #       Linear(n_hidden â†’ d_model)\n        # 3. Create THREE SublayerConnection modules\n        self.self_attn = MultiHeaderAttention(d_model, dropout)\n        self.src_attn = MultiHeaderAttention(d_model, dropout)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, n_hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(n_hidden, d_model)\n        )\n        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n\n    def forward(self, x, memory, tgt_mask, src_mask):\n        \"\"\"\n        STUDENT TASK:\n\n        Decoder forward pass:\n\n        Given:\n            x: (batch, tgt_len, d_model)\n            memory: encoder outputs (batch, src_len, d_model)\n            tgt_mask: future mask (for self-attention)\n            src_mask: padding mask (for encoder-decoder attention)\n\n        Steps:\n        1. Masked self-attention:\n               sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n\n        2. Encoder-decoder attention: broadly, target attends to memory\n               sublayer[1](output, lambda x: self.src_attn(memory, memory, x, src_mask))\n\n        3. Feed-forward network:\n               sublayer[2](output, self.feed_forward)\n\n        Return:\n            Final transformed representation.\n        \"\"\"\n        x = self.sublayer[0](x, lambda x_in: self.self_attn(x_in, x_in, x_in, tgt_mask))\n        x = self.sublayer[1](x, lambda x_in: self.src_attn(memory, memory, x_in, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n","metadata":{"trusted":true,"id":"TQMOoIJgt6v7","execution":{"iopub.status.busy":"2025-12-25T09:28:15.320179Z","iopub.execute_input":"2025-12-25T09:28:15.320430Z","iopub.status.idle":"2025-12-25T09:28:15.340212Z","shell.execute_reply.started":"2025-12-25T09:28:15.320410Z","shell.execute_reply":"2025-12-25T09:28:15.339500Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## The Transfomer\n\nNow we have all the pieces for the transformer. Here, the encoder and decoder are instanciated, the masks that prevent looking at irrelevant (pad tokens) or illegal (looking ahead) content are generated. A linear layer outputs the final guesses of the word distributions for the whole sequence. Note that the Softmax functions are not implemented. This is because the Crossentropy Loss expects the raw output.","metadata":{"id":"tY3tOCM2t6v7"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n############################################\n# STUDENT TASK: Implement the Transformer\n############################################\nclass Transformer(nn.Module):\n    \"\"\"\n    Full Transformer model composed of:\n    - Encoder\n    - Decoder\n    - Final linear projection to vocabulary logits\n\n    Students must implement:\n    1. Initialization of encoder, decoder, and final output layer.\n    2. Padding mask for both encoder and decoder.\n    3. Causal target mask (for autoregressive decoding).\n    4. Forward pass combining all components.\n    \"\"\"\n\n    def __init__(self, d_model, n_input_vocab, n_target_vocab, n_hidden, n_layers, dropout):\n        super().__init__()\n\n        # STUDENT TASK:\n        # 1. Instantiate an Encoder with the given arguments.\n        # 2. Instantiate a Decoder.\n        # 3. Add a final Linear layer projecting decoder outputs to target vocabulary size.\n        self.encoder = Encoder(n_input_vocab, d_model, n_hidden, n_layers, dropout)\n        self.decoder = Decoder(n_target_vocab, d_model, n_hidden, n_layers, dropout)\n        self.output_layer = nn.Linear(d_model, n_target_vocab)\n\n    ############################################\n    # STUDENT TASK: Padding Mask\n    ############################################\n    def get_pad_mask(self, seq):\n        \"\"\"\n        Build a padding mask for sequences.\n\n        Students must:\n        - Return a mask of shape (batch, 1, seq_len)\n        - Mask = True where token != PAD_token\n        - Mask = False where token == PAD_token\n        \"\"\"\n        return (seq != PAD_token).unsqueeze(1)\n\n    ############################################\n    # STUDENT TASK: Target Mask (Causal Mask)\n    ############################################\n    def get_target_mask(self, target_seq):\n        \"\"\"\n        Build a causal mask to prevent attention to future tokens.\n\n        Students must:\n        - Use torch.tril() to create a lower-triangular matrix\n        - Shape: (1, tgt_len, tgt_len)\n        - Return a boolean mask suitable for attention\n        \"\"\"\n        tgt_len = target_seq.size(1)\n        # Create a lower triangular matrix of ones\n        mask = torch.tril(torch.ones(tgt_len, tgt_len, device=target_seq.device)).bool()\n        return mask.unsqueeze(0) # Add a batch dimension (1, tgt_len, tgt_len)\n\n    ############################################\n    # STUDENT TASK: Forward Pass\n    ############################################\n    def forward(self, input_seq, target_seq):\n        \"\"\"\n        Complete the full forward pass of the Transformer.\n\n        Steps:\n        1. Compute source padding mask.\n        2. Compute target padding mask.\n        3. Compute causal mask and combine with padding mask.\n        4. Pass input_seq through the encoder.\n        5. Pass target_seq and encoder output into the decoder.\n        6. Apply the final linear projection to get vocab logits.\n\n        Returns:\n            Output logits of shape (batch, tgt_len, n_target_vocab)\n        \"\"\"\n        # 1. Compute source padding mask.\n        src_mask = self.get_pad_mask(input_seq)\n\n        # 2. Compute target padding mask.\n        tgt_pad_mask = self.get_pad_mask(target_seq)\n\n        # 3. Compute causal mask and combine with padding mask.\n        tgt_causal_mask = self.get_target_mask(target_seq)\n        # Combine the padding mask and causal mask for the decoder's self-attention\n        decoder_self_attn_mask = tgt_pad_mask & tgt_causal_mask\n\n        # 4. Pass input_seq through the encoder.\n        encoder_output = self.encoder(input_seq, src_mask)\n\n        # 5. Pass target_seq and encoder output into the decoder.\n        # src_mask is used here as enc_dec_mask to mask the encoder outputs when the decoder attends to them.\n        decoder_output = self.decoder(target_seq, encoder_output, decoder_self_attn_mask, src_mask)\n\n        # 6. Apply the final linear projection to get vocab logits.\n        output = self.output_layer(decoder_output)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.341082Z","iopub.execute_input":"2025-12-25T09:28:15.341287Z","iopub.status.idle":"2025-12-25T09:28:15.355275Z","shell.execute_reply.started":"2025-12-25T09:28:15.341263Z","shell.execute_reply":"2025-12-25T09:28:15.354604Z"},"trusted":true,"id":"rYw0WxdGt6v7"},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Training & Random Tests","metadata":{"id":"VCydiOR9t6v8"}},{"cell_type":"markdown","source":"It's training time. First we shuffle the dataset and split it into training and test partitions. Then we instatiate the model and set up the optimizer and scheduler. Finally we run the training loop. We get a hot cup of coffee, lean back and look mesmerized at the ever decreasing loss. â˜•ï¸","metadata":{"id":"YTn3s81Ot6v8"}},{"cell_type":"code","source":"#splitting the data into train and test datasets\n\nnp.random.shuffle(pairs)\ntrain = pairs[:-1000]\ntest = pairs[-1000:]\n\n#the model with the parameter-values of the paper\ntransformer1 = Transformer(d_model=512,\n                            n_input_vocab=input_lang.n_words,\n                            n_target_vocab= output_lang.n_words,\n                            n_hidden = 2048,\n                            n_layers=6,\n                            dropout=0.1).to(device)\n\ntransformer1 = nn.DataParallel(transformer1)\n\nfor p in transformer1.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:15.356067Z","iopub.execute_input":"2025-12-25T09:28:15.356297Z","iopub.status.idle":"2025-12-25T09:28:17.073954Z","shell.execute_reply.started":"2025-12-25T09:28:15.356271Z","shell.execute_reply":"2025-12-25T09:28:17.073352Z"},"trusted":true,"id":"H4WB3AN1t6v8"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Optimizer\n\n> We used the Adam optimizer [20] with Î²1 = 0.9, Î²2 = 0.98 and \u000f = 10âˆ’9\n. We varied the learning\nrate over the course of training, according to the formula:<br>\n`lrate = d_model**âˆ’0.5 model Â· min(step_numâˆ’0.5, step_num Â· warmup_stepsâˆ’1.5)` (3)<br>\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.","metadata":{"id":"NngcQYUtt6v8"}},{"cell_type":"code","source":"#the optimizer\nlr= 1\nopt1 = optim.Adam(transformer1.parameters(),lr=lr, betas=(0.9, 0.98), eps=1e-09)\n\ndef lr_rate(step_num, d_model, factor, warmup_steps):\n    step_num =max(1,step_num)\n    return factor * (\n        d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n    )\n\nlr_scheduler = LambdaLR(\n    optimizer=opt1,\n    lr_lambda=lambda step_num: lr_rate(\n        step_num, 512, factor=1, warmup_steps=4000\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:17.074784Z","iopub.execute_input":"2025-12-25T09:28:17.075051Z","iopub.status.idle":"2025-12-25T09:28:21.121005Z","shell.execute_reply.started":"2025-12-25T09:28:17.075019Z","shell.execute_reply":"2025-12-25T09:28:21.120439Z"},"trusted":true,"id":"7MhgIepkt6v8"},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Training and evaluation functions\n\nLet's define a function to test the model with random samples from the test set. This will make the learning process more intuitive. Ultimately, we want to see the model translate something it has not seen before and the better the quality of this.","metadata":{"id":"F38vJHEqt6v8"}},{"cell_type":"code","source":"def pred(input_seq, model):\n    outputs = [SOS_token]\n\n    loss = 0\n    for i in range(MAX_LENGTH):\n        target_seq = torch.tensor([outputs],device=device)\n        output = model(input_seq,target_seq)\n        probs = F.softmax(output,dim=2)\n        word_pred = torch.argmax(probs[:,-1,:],dim=1)\n\n        outputs.append(word_pred.item())\n\n        if word_pred.item()== EOS_token:\n            break\n\n    return outputs[1:]\n\ndef random_model_testing(n_examples,model):\n    batch_sz=1\n    test_samples = [random.choice(test) for i in range(n_samples)]\n    print(\"Random Tests\")\n    print(\"*\"*30)\n    for i in range(0,len(test_samples[:n_examples]),batch_sz):\n        input_tensor, output_tensor = batch_from_pairs(test_samples[i:i+batch_sz])\n        out = pred(input_tensor, model)\n        print(\"Pred: \", \" \".join([output_lang.index2word[i] for i in out]), \"True: \",test_samples[i][1])\n    print(\"*\"*30)","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:21.121981Z","iopub.execute_input":"2025-12-25T09:28:21.122492Z","iopub.status.idle":"2025-12-25T09:28:21.128446Z","shell.execute_reply.started":"2025-12-25T09:28:21.122461Z","shell.execute_reply":"2025-12-25T09:28:21.127691Z"},"trusted":true,"id":"AGB2d4bat6v8"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def train_batch(input_seq, target_seq, model, optimizer,scheduler):\n    target, truth = target_seq[:,:-1], target_seq[:,1:]\n    pred = model(input_seq,target)\n\n    loss = F.cross_entropy(pred.view(-1,output_lang.n_words), truth.reshape(-1),reduction='sum',label_smoothing=0.1)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    scheduler.step()\n\n    return loss.item()","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:21.129210Z","iopub.execute_input":"2025-12-25T09:28:21.129425Z","iopub.status.idle":"2025-12-25T09:28:21.152030Z","shell.execute_reply.started":"2025-12-25T09:28:21.129405Z","shell.execute_reply":"2025-12-25T09:28:21.151450Z"},"trusted":true,"id":"bRyFYheXt6v9"},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Let's train our transfomer!\n\nBellow is a simple training loop. Each epoch we feed the input and target batches to the model, calculate the loss and backpropagate.","metadata":{"id":"uzz2qu7_t6v9"}},{"cell_type":"code","source":"n_samples=21000\nepochs =200\nbatch_sz=128\n\nfor e in range(epochs):\n    loss = 0\n    train_samples = [random.choice(train) for i in range(n_samples)]\n    for i in range(0,len(train_samples),batch_sz):\n        input_tensor, output_tensor = batch_from_pairs(train_samples[i:i+batch_sz])\n        loss += train_batch(input_tensor, output_tensor, transformer1, opt1,lr_scheduler)\n    print(f\"Epoch {e}/{epochs} | loss: {round(loss/n_samples,2)} | learning rate: {round(lr_scheduler.get_last_lr()[0],6)}\")\n    #random testing\n    if e%25==0:\n        random_model_testing(10,transformer1)","metadata":{"execution":{"iopub.status.busy":"2025-12-25T09:28:21.152947Z","iopub.execute_input":"2025-12-25T09:28:21.153243Z","execution_failed":"2025-12-25T16:16:33.000Z"},"trusted":true,"id":"RkT5Fm9lt6v9","outputId":"f57cefc4-3d71-4c8d-e929-3a20ceb7f88f"},"outputs":[{"name":"stdout","text":"Epoch 0/200 | loss: 465.1 | learning rate: 2.9e-05\nRandom Tests\n******************************\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  the get welcomes this and observes that awareness raising measures should be taken to make sure that the prohibition of tax deductibility of expenses linked to bribery is fully implemented in practice .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:   governments should repeal any laws and regulations which discriminate against minorities including the roma and non settled communities in terms of political representation .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  definitions contributing country a country that participates in the montreal protocol and that must make mandatory annual contributions to the multilateral fund and or that participates in bilateral agreements with other countries partners .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  board membership includes leaders of the fishing industry and aboriginal organisations as well as of other key associations aquaculturists recreational fishers and environmental groups .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  rocanville division potash potash corp . of sask .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  males called bulls weigh as much as kg and have massive necks and foreparts .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  the rates decrease with increase in h .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:   the office publishes these edited judgments for readers information .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  support for interdepartmental coordination the alberta region contracted a consultant to assist the community in developing a better relationship with federal departments .\nPred:  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD True:  capital expenses can include \n******************************\nEpoch 1/200 | loss: 321.64 | learning rate: 5.8e-05\nEpoch 2/200 | loss: 212.7 | learning rate: 8.6e-05\nEpoch 3/200 | loss: 195.91 | learning rate: 0.000115\nEpoch 4/200 | loss: 190.97 | learning rate: 0.000144\nEpoch 5/200 | loss: 187.02 | learning rate: 0.000173\nEpoch 6/200 | loss: 183.49 | learning rate: 0.000202\nEpoch 7/200 | loss: 178.73 | learning rate: 0.000231\nEpoch 8/200 | loss: 176.68 | learning rate: 0.000259\nEpoch 9/200 | loss: 172.51 | learning rate: 0.000288\nEpoch 10/200 | loss: 168.9 | learning rate: 0.000317\nEpoch 11/200 | loss: 165.76 | learning rate: 0.000346\nEpoch 12/200 | loss: 162.71 | learning rate: 0.000375\nEpoch 13/200 | loss: 159.99 | learning rate: 0.000404\nEpoch 14/200 | loss: 156.2 | learning rate: 0.000432\nEpoch 15/200 | loss: 153.39 | learning rate: 0.000461\nEpoch 16/200 | loss: 151.03 | learning rate: 0.00049\nEpoch 17/200 | loss: 147.74 | learning rate: 0.000519\nEpoch 18/200 | loss: 145.69 | learning rate: 0.000548\nEpoch 19/200 | loss: 143.69 | learning rate: 0.000576\nEpoch 20/200 | loss: 142.26 | learning rate: 0.000605\nEpoch 21/200 | loss: 140.23 | learning rate: 0.000634\nEpoch 22/200 | loss: 139.54 | learning rate: 0.000663\nEpoch 23/200 | loss: 137.83 | learning rate: 0.000692\nEpoch 24/200 | loss: 136.45 | learning rate: 0.000688\nEpoch 25/200 | loss: 135.31 | learning rate: 0.000675\nRandom Tests\n******************************\nPred:   to close francophone communities particularly in highly minority areas by helping communities develop a strategy for the strategy of the communities . EOS True:   counter the assimilation of francophones particularly in strongly minority areas by helping the communities to develop a refrancization strategy .\nPred:  support for consultation the regional office of alberta has been held by a consultant to assist the community in response to the federal departments . EOS True:  support for interdepartmental coordination the alberta region contracted a consultant to assist the community in developing a better relationship with federal departments .\nPred:   highlights of the road map development process EOS True:   key nuggets extracted from the roadmap process\nPred:  regional activities are the voluntary activities . EOS True:  a regional programme covers multilateral activities .\nPred:  in the early s leaders the world leaders have decided to peace and a more effective environment . EOS True:  when the millennium opened world leaders pledged to seek peace the end of poverty and a cleaner environment .\nPred:  post charge expenses of internal meetings figures differentiated appropriations outturn  EOS True:  remarks this appropriation is intended to cover the costs of the beverages refreshments and occasional light meals served at meetings held by the institution .\nPred:  the previous experience is recognized by the grant of a level increase in each year of experience in education where the appointment has been created before the appointment has been placed on an bargaining unit . EOS True:  credit for previous experience experience is recognized by the granting of one increment for each acceptable year of teaching or counselling experience prior to appointment to a position in the bargaining unit .\nPred:  however the following comments are  EOS True:  however the following observations can be made \nPred:   study on the right to the white house report of the high commissioner of the human rights of human rights doc . EOS True:   . study on the right to the truth report of the office of the united nations high commissioner for human rights un doc .\nPred:  notice of the application was published on april in newspapers of the regions and posted to the post of fort operandi and house of commons . EOS True:  notice of the application was published on april and in the newspapers of the areas concerned and posted at the local post offices of moosonee fort albany kashechewan attawapiskat and rupert house .\n******************************\nEpoch 26/200 | loss: 133.88 | learning rate: 0.000662\nEpoch 27/200 | loss: 132.55 | learning rate: 0.00065\nEpoch 28/200 | loss: 131.17 | learning rate: 0.000639\nEpoch 29/200 | loss: 131.05 | learning rate: 0.000628\nEpoch 30/200 | loss: 129.62 | learning rate: 0.000618\nEpoch 31/200 | loss: 128.22 | learning rate: 0.000608\nEpoch 32/200 | loss: 127.32 | learning rate: 0.000599\nEpoch 33/200 | loss: 127.42 | learning rate: 0.00059\nEpoch 34/200 | loss: 126.48 | learning rate: 0.000582\nEpoch 35/200 | loss: 125.77 | learning rate: 0.000573\nEpoch 36/200 | loss: 124.45 | learning rate: 0.000566\nEpoch 37/200 | loss: 123.56 | learning rate: 0.000558\nEpoch 38/200 | loss: 122.42 | learning rate: 0.000551\nEpoch 39/200 | loss: 123.96 | learning rate: 0.000544\nEpoch 40/200 | loss: 122.38 | learning rate: 0.000537\nEpoch 41/200 | loss: 121.31 | learning rate: 0.000531\nEpoch 42/200 | loss: 120.95 | learning rate: 0.000525\nEpoch 43/200 | loss: 120.54 | learning rate: 0.000519\nEpoch 44/200 | loss: 120.45 | learning rate: 0.000513\nEpoch 45/200 | loss: 119.74 | learning rate: 0.000507\nEpoch 46/200 | loss: 118.8 | learning rate: 0.000502\nEpoch 47/200 | loss: 117.7 | learning rate: 0.000497\nEpoch 48/200 | loss: 119.21 | learning rate: 0.000492\nEpoch 49/200 | loss: 117.53 | learning rate: 0.000487\nEpoch 50/200 | loss: 117.59 | learning rate: 0.000482\nRandom Tests\n******************************\nPred:  e . w . of the author s . file c .m . painting . . . . memory . . . . . . transaction no . . . . EOS True:  instructions for action in case of emergency aug w .d . g .s . h .q . nd div . august appendix nd div .\nPred:  canadian winter games were held in the olympic winter games and was recently completed during the games games meeting . EOS True:  canadian olympic winter athletes posted their best ever games finish in placing third overall with medals .\nPred:  encourage them to discuss their findings by discussing the indices of the description to support their decisions . EOS True:  encourage partners to discuss their conclusions by citing evidence from the description to support their decisions .\nPred:  during the months they fought against all of the elements since the freezing freezing cold and cold before the cold as well as the biochemical rain . EOS True:  for two months they battled everything from freezing cold temperatures to constant rain to extreme heat .\nPred:   national park of the peninsula bruce EOS True:   bruce peninsula national park\nPred:  as directed in crtc s letter dated december aliant telecom files its best estimate of the trial date of the digital archive for the communities listed in the company s october document enhancement document . document .doc kb telecom inc . for telus communications inc . description  EOS True:  as directed in crtc s letter dated december aliant telecom files its best estimate of completion date of the digital upgrades for the communities listed in the company s october letter network enhancement . document .doc kb aliant telecom inc . for newtel communications inc . description \nPred:   canada s performance p . EOS True:   canada s performance p . . a managing food safety risks ongoing activities \nPred:  access to the labour market in the member state of residence will be limited the first two years . EOS True:  access to the member state of residence s labour market will be restricted for the first two years .\nPred:  the sodium potassium effect was shown that the potassium was increased and that the potassium decreases in days to the days of the PAD were recovered . EOS True:  we demonstrate that intracellular sodium increases and potassium decreases from days onwards .\nPred:  a five year approach involving a formula of shared representation was put in place in . EOS True:  a five year co management arrangement with a sharing formula was put in place in .\n******************************\nEpoch 51/200 | loss: 117.1 | learning rate: 0.000477\nEpoch 52/200 | loss: 116.77 | learning rate: 0.000473\nEpoch 53/200 | loss: 115.79 | learning rate: 0.000468\nEpoch 54/200 | loss: 115.57 | learning rate: 0.000464\nEpoch 55/200 | loss: 115.23 | learning rate: 0.00046\nEpoch 56/200 | loss: 115.19 | learning rate: 0.000456\nEpoch 57/200 | loss: 114.5 | learning rate: 0.000452\nEpoch 58/200 | loss: 114.89 | learning rate: 0.000448\nEpoch 59/200 | loss: 114.43 | learning rate: 0.000444\nEpoch 60/200 | loss: 114.14 | learning rate: 0.000441\nEpoch 61/200 | loss: 112.99 | learning rate: 0.000437\nEpoch 62/200 | loss: 112.79 | learning rate: 0.000433\nEpoch 63/200 | loss: 112.2 | learning rate: 0.00043\nEpoch 64/200 | loss: 112.61 | learning rate: 0.000427\nEpoch 65/200 | loss: 111.87 | learning rate: 0.000423\nEpoch 66/200 | loss: 111.91 | learning rate: 0.00042\nEpoch 67/200 | loss: 111.14 | learning rate: 0.000417\nEpoch 68/200 | loss: 110.84 | learning rate: 0.000414\nEpoch 69/200 | loss: 110.99 | learning rate: 0.000411\nEpoch 70/200 | loss: 110.21 | learning rate: 0.000408\nEpoch 71/200 | loss: 109.64 | learning rate: 0.000405\nEpoch 72/200 | loss: 110.08 | learning rate: 0.000403\nEpoch 73/200 | loss: 109.4 | learning rate: 0.0004\nEpoch 74/200 | loss: 109.41 | learning rate: 0.000397\nEpoch 75/200 | loss: 109.17 | learning rate: 0.000395\nRandom Tests\n******************************\nPred:   trade firms and consultation offices EOS True:   business and consulting firms\nPred:  the vendor is trying to obtain approval from the buyer to amend the purchase and sale agreement to take account of gst by increasing the purchase price of p . . EOS True:  the vendor attempted but the purchaser did not agree to amend the purchase and sale agreement to account for the gst by increasing the purchase price by .\nPred:  make sure you can use your visit . EOS True:  photocopy it and take copies with you while you re house hunting .\nPred:  there is a need to increase access to the sport facilities of the department of national defence . EOS True:  increase access to department of national defense sport facilities .\nPred:   restore a safety sense of our cities and neighbourhood by showing more severe and consistent with street gangs and crime committed with fire EOS True:   ensure safe communities by tackling gun gang and drug crime\nPred:  in addition million was spent on transportation million . million for accommodation and . million for other materials such as access rights . EOS True:  another . million . percent was spent on transportation . million . percent on food . million . percent on accommodation and . million . percent on other items such as entry fees .\nPred:  to make an application for the use of a claim for use by fax to the related results sheet and a statement that allows an to be made to a later date for each of the seven calendar days after the date of your outcome . EOS True:  therefore to request a rescore you must fax your test result sheet with the accompanying letter and a note requesting a rescore to the rsas section at within seven calendar days from the date of notification of your results .\nPred:  the quarterly analysis of the oag are based on data not . EOS True:  the ctc quarterly analyses are based on seasonally unadjusted data .\nPred:  to be collaboration the roles and responsibilities have been clearly established and the communication is to be effective . EOS True:  clearly defined roles and responsibilities and effective communication are imperative for any collaborative undertaking .\nPred:  first in the federal government key focus on human resource business services is being placed on financial resources for equipment . EOS True:  the initial areas of focus for shared services within the gc are transactional human resources financial materiel services and it services .\n******************************\nEpoch 76/200 | loss: 109.0 | learning rate: 0.000392\nEpoch 77/200 | loss: 108.85 | learning rate: 0.00039\nEpoch 78/200 | loss: 108.26 | learning rate: 0.000387\nEpoch 79/200 | loss: 108.09 | learning rate: 0.000385\nEpoch 80/200 | loss: 108.05 | learning rate: 0.000382\nEpoch 81/200 | loss: 108.04 | learning rate: 0.00038\nEpoch 82/200 | loss: 107.54 | learning rate: 0.000378\nEpoch 83/200 | loss: 107.74 | learning rate: 0.000375\nEpoch 84/200 | loss: 107.07 | learning rate: 0.000373\nEpoch 85/200 | loss: 106.47 | learning rate: 0.000371\nEpoch 86/200 | loss: 106.27 | learning rate: 0.000369\nEpoch 87/200 | loss: 106.53 | learning rate: 0.000367\nEpoch 88/200 | loss: 106.17 | learning rate: 0.000365\nEpoch 89/200 | loss: 105.2 | learning rate: 0.000363\nEpoch 90/200 | loss: 105.72 | learning rate: 0.000361\nEpoch 91/200 | loss: 105.96 | learning rate: 0.000359\nEpoch 92/200 | loss: 105.08 | learning rate: 0.000357\nEpoch 93/200 | loss: 105.32 | learning rate: 0.000355\nEpoch 94/200 | loss: 104.94 | learning rate: 0.000353\nEpoch 95/200 | loss: 104.71 | learning rate: 0.000351\nEpoch 96/200 | loss: 104.94 | learning rate: 0.000349\nEpoch 97/200 | loss: 104.42 | learning rate: 0.000348\nEpoch 98/200 | loss: 104.68 | learning rate: 0.000346\nEpoch 99/200 | loss: 103.94 | learning rate: 0.000344\nEpoch 100/200 | loss: 103.45 | learning rate: 0.000342\nRandom Tests\n******************************\nPred:  between and the government of canada provided . million in de a de in de a in de as a canadian government and . billion dollars to . million in for the development of the ca and la province . EOS True:  during the period the canadian government provided million to de havilland and . billion to canadair for development of the dash and the challenger respectively and to re capitalize the companies in preparation for their eventual return to the private sector .\nPred:  however the new data on the rate of consumption of country of country of first nations could be applied to projects that have an assessment of risks on reserve . EOS True:  despite this the new information on the rates of country food consumption among first nations could be applied to projects involving risk assessments on reserve .\nPred:  date of amendment top important notices EOS True:   user guide proactive disclosure proactive disclosure\nPred:   over the next five years i want that we will have a great deal of success for our credit in our societies said minister . EOS True:   i want the next five years to build on the success we ve already achieved stated the minister .\nPred:  at the beginning of the millennium world leaders are responding to peace poverty and a more unique environment . EOS True:  when the millennium opened world leaders pledged to seek peace the end of poverty and a cleaner environment .\nPred:  canadian wildlife conservation program in terrestrial birds some of the most known birds and sharp print in canada are found . EOS True:  canadian landbird conservation program landbirds include some of the most familiar and best loved birds in canada .\nPred:  new published publishing available in english  EOS True:  new editions available\nPred:  apply the criteria of the small independent business to determine whether the borrowers are limited to a maximum amount of loans of . per cent loans category . per cent loans act . EOS True:  apply the independent small business test to determine whether related borrowers are limited to a maximum aggregate outstanding loan of loan classes regs ss . there are four classes of csbf loan \nPred:  the PAD of the commissioner will also use a similar method to disseminate information to all affected parties . EOS True:  using similar means of communication the acsl in turn will ensure that the information is conveyed to all affected parties .\nPred:  members include heads of the fishing and aboriginal organizations and other major organizations fisheries and environmental groups . EOS True:  board membership includes leaders of the fishing industry and aboriginal organisations as well as of other key associations aquaculturists recreational fishers and environmental groups .\n******************************\nEpoch 101/200 | loss: 104.05 | learning rate: 0.000341\nEpoch 102/200 | loss: 103.64 | learning rate: 0.000339\nEpoch 103/200 | loss: 103.42 | learning rate: 0.000337\nEpoch 104/200 | loss: 103.45 | learning rate: 0.000336\nEpoch 105/200 | loss: 102.9 | learning rate: 0.000334\nEpoch 106/200 | loss: 103.0 | learning rate: 0.000333\nEpoch 107/200 | loss: 102.45 | learning rate: 0.000331\nEpoch 108/200 | loss: 102.32 | learning rate: 0.00033\nEpoch 109/200 | loss: 101.78 | learning rate: 0.000328\nEpoch 110/200 | loss: 101.73 | learning rate: 0.000327\nEpoch 111/200 | loss: 101.99 | learning rate: 0.000325\nEpoch 112/200 | loss: 102.22 | learning rate: 0.000324\nEpoch 113/200 | loss: 101.55 | learning rate: 0.000322\nEpoch 114/200 | loss: 100.73 | learning rate: 0.000321\nEpoch 115/200 | loss: 100.98 | learning rate: 0.000319\nEpoch 116/200 | loss: 101.27 | learning rate: 0.000318\nEpoch 117/200 | loss: 101.03 | learning rate: 0.000317\nEpoch 118/200 | loss: 100.89 | learning rate: 0.000315\nEpoch 119/200 | loss: 101.08 | learning rate: 0.000314\nEpoch 120/200 | loss: 100.59 | learning rate: 0.000313\nEpoch 121/200 | loss: 99.97 | learning rate: 0.000311\nEpoch 122/200 | loss: 100.81 | learning rate: 0.00031\nEpoch 123/200 | loss: 100.09 | learning rate: 0.000309\nEpoch 124/200 | loss: 100.04 | learning rate: 0.000308\nEpoch 125/200 | loss: 100.06 | learning rate: 0.000307\nRandom Tests\n******************************\nPred:  the geographical structure of the region is a problem itself water resources are remote from areas where water is required . EOS True:  physical geography of the country water resources away from area where water is needed .\nPred:   there are few financial assistance mechanisms available to support the initial development stages of a scenario  EOS True:   there are few avenues of direct financial assistance to support the initial development stages of a script \nPred:  it is important that those who live on the front and can see men other than the way the way that the canadian race is not the right and that it constitute a violation of human rights .  EOS True:  it is important that men appear in the vanguard precisely in order to tell other men that the path leading to domestic violence is not the right one and constitutes a violation of human rights . \nPred:  the rates of is now that canada s rates per day have very thin margins of age . EOS True:  charter rates in canada now vary between and per day meaning that bus companies operate on very thin margins .\nPred:  they have thus been able to qualify as for the risk of the individual or an individual s individuals involved with whom they are related . EOS True:  they therefore entail that the legal or natural persons to whom such warnings are associated are classified as at risk .\nPred:  detailed guidelines will be available in the sar nif nif manual available to the nss website at the beginning of the year . EOS True:  q when will the next call letter for new projects be issued ?\nPred:   total funds spent on projects sampled . a detailed cross site breakdown of sample size and survey is contained in appendix b . EOS True:  more specifically for each project the three largest dollar value transactions and one random transaction were selected for testing from each fiscal year .\nPred:   recognized as a national authority for the management of natural health products used in canada  EOS True:   acknowledged as the national authority for the regulation of natural health products used in canada \nPred:  to use the rss format you should download a new reading software text which is not included in a broadcast list thereby reducing the risk to use the ict content . EOS True:  because rss requires you to download a news reader you aren t subscribing to e mail lists which can increase your risk of getting unsolicited spam e mail .\nPred:  trade unions also rated the declaration enabling the majority of the respondents to identify and obtain the total confidentiality of their information . EOS True:  the unions also considered misleading the statement assuring employees of the complete confidentiality of the information .\n******************************\nEpoch 126/200 | loss: 99.47 | learning rate: 0.000305\nEpoch 127/200 | loss: 99.7 | learning rate: 0.000304\nEpoch 128/200 | loss: 98.96 | learning rate: 0.000303\nEpoch 129/200 | loss: 99.24 | learning rate: 0.000302\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#more random tests\nrandom_model_testing(20,transformer1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-25T16:16:33.001Z"},"id":"aa38QttRt6v9"},"outputs":[],"execution_count":null}]}